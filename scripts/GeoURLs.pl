#!/usr/bin/perl

# This script generates the minimum data required for serving
# a file via HTTP, apart from what is available via stat:
# url and mime-type.
#
# Since the url will be queried often, it is put into a 
# seperate table in the hope for more speed. Also, it might
# be that several URLs are poiting to the same file. To
# keep this option open, the choices are marked by an agent.
#
# Since the mime-type ('Content-Type') can never be
# fully accurately determined automatically, aka 
# "is open for interpretation", it will be stored in the 
# triple store "props".

use feature ':5.14';

use strict;
use warnings;
use diagnostics;

use Encode;
use Try::Tiny;

use IO::All;
use File::stat;
use Cwd qw(abs_path);

use Digest;
use DBI;

use Data::Dumper;

use threads;
use threads::shared;

my $AGENT = 'GeoURLs';
my $VERSION = '0.1-despens';

$| = 1; # turn on autoflush

my $counter = 0;        # Makes sure that a db commit is issued every $limit files
my $limit = 1024;

my $branch = $ARGV[0] ? ".$ARGV[0]" : "";

# This list was generated before, see '012-ingest.sh'
my $listfile_name = "$ENV{GEO_LOGS}/db-files.txt$branch";

open(INPUT, "< $listfile_name");
                                                

# All analyzing and ingesting scripts are roughly the same:
#
# First, a text list containing file names and file ids from
# the db is generated by the db. This happenes outside of
# the script. The main reason for this is increased speed over
# moving around queries and cursors inside the db server
# simultaneously for reading and writing. With a few million
# rows to handle, this adds up dramatically. -- Of course
# depending on your setup. :)
#
# Next, the script runs through the lines of the text file,
# analyzes the files listed there, and generates new data
# about them. When a certein amout of new rows has accumulated, 
# they are INSERTed into the db and committed. Again, committing
# not every line mainly gives a big speed advantage. INSERTing
# happens in a separate thread so new files can be analyzed
# while the result packages are written.

my (@urls, @props);

while(<INPUT>) {
    chomp;

    my ($id, $path) = $_ =~ m/(.+?)\|(.+)/;

    if ($counter==0) {
        @urls=();
        @props=();
    }

    say "$branch $path";

    my $row = {
        id   => $id,
        path => $path
    };
    
    # url

    my ($url) = $row->{path} =~ m|^[^/]+/(.+)$|;

    my $record_url = {
        url => $url,
        file_id => $row->{id},
        agent => "$AGENT $VERSION",
    };

    my $sha1_id = Digest->new('SHA-1');

    map { $sha1_id->add($_) } (
        $record_url->{url},
        $record_url->{file_id},
        $record_url->{agent}, 
    );

    $record_url->{id} = $sha1_id->hexdigest;

    push @urls, $record_url;

    # props (mime type)
    # This requires the "Gnome Virtual File System", gvfs.
    # This software guesses mime-types much better than the
    # classic "file", recognizing for example MS Office
    # documents, MIDI-files, etc ... PROTIP! This
    # thing is present on any Ubuntu Desktop install.

    my $gvfs_info = fexec('gvfs-info', '-a', 'standard::content-type', $ENV{GEO_ARCHIVE}.'/'.$row->{path});

    my ($mime_type) = $gvfs_info =~ m/standard::content-type: (\S+)/;

    my $record_prop = {
        file_id   => $row->{id},
        rel       => 'HTTP Content-Type',
        obj       => $mime_type,
        agent     => "gvfs-info via $AGENT $VERSION",
    };

    $sha1_id = Digest->new('SHA-1');

    map { $sha1_id->add($_) } (
        $record_prop->{file_id},
        $record_prop->{rel},
        $record_prop->{obj}, 
        $record_prop->{agent}, 
    );

    $record_prop->{id} = $sha1_id->hexdigest;

    push @props, $record_prop;

    # print Dumper($record_prop);

    $counter++;
    if($counter==$limit or eof(INPUT)) {
       ingest(\@urls, \@props);
        $counter = 0;
    }

}

# We be done with that list file!
system('rm', '-v', $listfile_name);



sub ingest {

    my ($urls_pointer, $props_pointer) = @_;
    my @urls = @$urls_pointer;
    my @props = @$props_pointer;
    
    my $dbh = DBI->connect("dbi:Pg:dbname=$ENV{GEO_DB_DB}", $ENV{GEO_DB_USER}, $ENV{GEO_DB_PASSWD}, {RaiseError => 1, AutoCommit => 0});

    # Preparing SQL-Statements

    my $db_insert_url = $dbh->prepare(qq(
        INSERT INTO urls (id, url, file_id, agent) 
        VALUES           (?,  ?,   ?,       ?);
    ));

    my $db_insert_prop = $dbh->prepare(qq(
        INSERT INTO props (id, file_id, rel, obj, agent)
        VALUES            (?,  ?,       ?,   ?,   ?);
    ));

    for my $record_url (@urls) {
        print Dumper($record_url);
        try {
            # ram stuff into database!
            $db_insert_url->execute(
                $record_url->{id},
                $record_url->{url},
                $record_url->{file_id},
                $record_url->{agent}, 
            );
        } 

        catch {
            # should be writing something meaningful to STDERR
            say 'SKIPPING URL';
        };

    }
    undef @urls;
    undef $urls_pointer;

    for my $record_prop (@props) {
        print Dumper($record_prop);
        try {
            # ram stuff into database!
            $db_insert_prop->execute(
                $record_prop->{id},
                $record_prop->{file_id},
                $record_prop->{rel},
                $record_prop->{obj},
                $record_prop->{agent}, 
            );
        } 

        catch {
            # should be writing something meaningful to STDERR
            say 'SKIPPING PROP';
        };
    }
    undef @props;
    undef $props_pointer;

    $dbh->commit;
    $dbh->disconnect;

    return 1;

}

sub fexec {
    my $pid = open(FORK, "-|");
    my @result;
    # the fork
    if ($pid==0) { 
        exec(@_)
            or die "no exec: $!";
        exit;
    }
    # main process
    else {         
        @result = <FORK>;
    }
    close(FORK);
    return join('', @result);
}